{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Objective</h1>\n",
    "See, for each column in data we care about, which sites have null values across entire columns; this will tell us if perhaps there are sites with sensors that are completely down for certain measurements at certain times or across the entire year (2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Make sure to change paths before uploading to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_path = \"/project/lindner/moving/summer2018/2019/descriptive-output/exploration/\"\n",
    "#on cluster\n",
    "data = pd.read_csv('/project/lindner/moving/summer2018/Data_structure_3/Data_2000.csv')\n",
    "#on local\n",
    "#data = pd.read_csv('D:/programming-no-gdrive/air-pollution/data-sample/100k/data00_100000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>See data columns</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Output all AQS Codes (suspect some bad data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(descr_path+'codes.txt', 'w') as file:\n",
    "    for aqs in data['AQS_Code'].unique():\n",
    "        file.write(str(aqs)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Drop unneeded columns</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['redraw', 'co', 'co_flag', 'humid', 'humid_flag', 'pm25', 'pm25_flag', 'so2', 'so2_flag', 'solar', 'solar_flag', 'temp', 'temp_flag', 'dew', 'dew_flag', 'winddir_flag', 'windspd_flag', 'no_flag', 'no2_flag', 'nox_flag', 'o3_flag'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Add time features (Hour, Day, Month). Code from format-mark in notebooks/format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour']  = pd.to_datetime(data['epoch'], unit='s').dt.hour\n",
    "data['day'] = pd.to_datetime(data['epoch'], unit='s').dt.day\n",
    "data['month'] = pd.to_datetime(data['epoch'], unit='s').dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Drop all sites other than the 71 \"good\" sites Chinmay found with at least one data point in a relevant column per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On cluster\n",
    "valid_sites = pd.read_csv(\"/project/lindner/moving/summer2018/2019/descriptive-output/Sites_with_monthly_data.csv\")\n",
    "#On local\n",
    "#valid_sites = pd.read_csv(\"D:/programming-no-gdrive/DASH/Air Pollution/descriptive-output/Sites_with_monthly_data.csv\")\n",
    "data = data.loc[data['AQS_Code'].isin(valid_sites['site'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just doing hours first\n",
    "#This does hours as rows\n",
    "print(\"Working with Chinmay's 71 \\\"valid\\\" sites with at least one data point per month on selected columns \\n\")\n",
    "all_site = pd.DataFrame()\n",
    "for aqs_code in list(data['AQS_Code'].unique()):\n",
    "    site_records = data[data[\"AQS_Code\"] == aqs_code]\n",
    "    print(\"Data for site: \"+str(aqs_code)+ '\\n')\n",
    "    this_aqs_null_tallies = pd.DataFrame()\n",
    "    for col in ['no', 'no2', 'nox', 'o3']:\n",
    "        #pol_tallies = pd.DataFrame()\n",
    "        hour_tallies = {}\n",
    "        print(\"Pollutant: \"+col+\"\\n\")\n",
    "        for month in range(1,13):\n",
    "            #fill out\n",
    "            for day in range(1,32):\n",
    "                #fill out \n",
    "                for hour in range(0,24):\n",
    "                    if hour not in hour_tallies: \n",
    "                        hour_tallies[hour] = 0\n",
    "                    else:\n",
    "                        this_hour = site_records[(site_records['day']==day) & (site_records['hour']==hour) & (site_records['month']==month)]\n",
    "                        if this_hour.empty == False:\n",
    "                            hour_isnull = this_hour[col].isnull().all()\n",
    "                            if hour_isnull == True:\n",
    "                                hour_tallies[hour]+=1\n",
    "        for hour in range(0,24):\n",
    "            print(\"Nulls for hour \"+str(hour)+\": \"+str(hour_tallies[hour])+\"\\n\")\n",
    "        hour_tallies = pd.Series(hour_tallies)\n",
    "        this_aqs_null_tallies[col] = hour_tallies\n",
    "        col_newname = str(aqs_code)+\"-\"+col\n",
    "        all_site[col_newname] = hour_tallies\n",
    "        #print(pd.Series(hour_tallies))\n",
    "        #this_aqs_null_tallies\n",
    "        #print(\"Hours with no data: \")\n",
    "    this_aqs_null_tallies.to_csv(descr_path+aqs_code+\"-hour_nulls-2000.csv\", index=False)\n",
    "all_site.to_csv(descr_path+'all_sites_null_hour_tallies_2000.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Just doing hours first\n",
    "#This does hours as COLUMNS\n",
    "all_site_hour_null_tallies = pd.DataFrame()\n",
    "for aqs_code in list(data['AQS_Code'].unique()):\n",
    "    site_records = data[data[\"AQS_Code\"] == aqs_code]\n",
    "    print(\"Data for site: \"+str(aqs_code)+ '\\n')\n",
    "    this_aqs_null_tallies = pd.DataFrame()\n",
    "    for col in ['no', 'no2', 'nox', 'o3']:\n",
    "        #pol_tallies = pd.DataFrame()\n",
    "        hour_tallies = {}\n",
    "        print(\"Pollutant: \"+col+\"\\n\")\n",
    "        for month in range(1,13):\n",
    "            #fill out\n",
    "            for day in range(1,32):\n",
    "                #fill out \n",
    "                for hour in range(0,24):\n",
    "                    if hour not in hour_tallies: \n",
    "                        hour_tallies[hour] = 0\n",
    "                    else:\n",
    "                        this_hour = site_records[(site_records['day']==day) & (site_records['hour']==hour) & (site_records['month']==month)]\n",
    "                        if this_hour.empty == False:\n",
    "                            hour_isnull = this_hour[col].isnull().all()\n",
    "                            if hour_isnull == True:\n",
    "                                hour_tallies[hour]+=1\n",
    "        for hour in range(0,24):\n",
    "            print(\"Nulls for hour \"+str(hour)+\": \"+str(hour_tallies[hour])+\"\\n\")\n",
    "        this_aqs_null_tallies[col] = pd.Series(hour_tallies)\n",
    "        #print(pd.Series(hour_tallies))\n",
    "        #this_aqs_null_tallies\n",
    "        #print(\"Hours with no data: \")\n",
    "    this_aqs_null_tallies.to_csv(descr_path+aqs_code+\"-hour_nulls-2000.csv\", index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
