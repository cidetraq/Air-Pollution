{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do top-bottom mean imputation: \n",
    "I think you have to iterate through the rows (slow) -> try itertuples for faster version?\n",
    "Copy original dataframe. \n",
    "top_bottomed = X\n",
    "operate on top_bottomed\n",
    "Iterate until you reach a nan\n",
    "counter-flag \"nan\" set to 0\n",
    "Update the \"bottom\" variable each time\n",
    "(If nan == 0:\n",
    " update bottom)\n",
    "\"top\" variable empty ( = \"\")\n",
    "When finding a nan: \n",
    "flag \"nan\" += 1\n",
    "keep going, don't update bottom\n",
    "if nan == 1:\n",
    "    set nan_begin_index = current index\n",
    "When finding the first non-nan after nan(s):\n",
    "current = non-nan -> \n",
    "if nan>= 1:\n",
    "    top = current\n",
    "    mean = mean(top,bottom)\n",
    "    for all rows between nan_begin_index and current index, change nan values to just-found mean\n",
    "    reset bottom to current and top to blank string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model making below\n",
    "#Use several strategies: top-bottom mean, neural network, \n",
    "#use srun interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question is, is the distribution of missing data different than the distribution of data that is present? So is it even valid to do imputation on the missing data when it could be differently shaped due to good reasons? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plac.annotations(\n",
    "    input_path=(\"Path containing the data files to ingest\", \"option\", \"p\", str),\n",
    "    input_prefix=(\"{$prefix}year.csv\", \"option\", \"P\", str),\n",
    "    input_suffix=(\"year{$suffix}.csv\", \"option\", \"S\", str),\n",
    "    output_path=(\"Path to write the resulting numpy sequences / transform cache\", \"option\", \"o\", str),\n",
    "    year_begin=(\"First year to process\", \"option\", \"b\", int),\n",
    "    year_end=(\"Year to stop with\", \"option\", \"e\", int),\n",
    "    fillgps=(\"Add correct GPS information because it is often missing in Data_structure_3\", \"flag\", \"G\"),\n",
    "    naninvalid=(\"Set invalid col entries to nan\", \"flag\", \"N\"),\n",
    "    dropnan=(\"Drop nan rows\", \"flag\", \"D\"),\n",
    "    masknan=(\"Mask nan rows\", \"option\", \"M\", float),\n",
    "    fillnan=(\"Fill nan rows\", \"option\", \"F\", float),\n",
    "    aqsnumerical=(\"Convert AQS code to numerical\", \"flag\", \"A\"),\n",
    "    houston=(\"Only run for Houston sites\", \"flag\", \"H\"),\n",
    "    chunksize=(\"Process this many records at one time\", \"option\", 'C', int)\n",
    ")\n",
    "def main(input_path: str = '/project/lindner/air-pollution/level3_data/',\n",
    "         input_prefix: str = \"Data_\",\n",
    "         input_suffix: str = \"\",\n",
    "         output_path: str = '/project/lindner/air-pollution/current/2019/data-formatted/houston',\n",
    "         year_begin: int = 2000,\n",
    "         year_end: int = 2018,\n",
    "         fillgps: bool = False,\n",
    "         naninvalid: bool = False,\n",
    "         dropnan: bool = False,\n",
    "         masknan: float = None,\n",
    "         fillnan: float = None,\n",
    "         aqsnumerical: bool = False,\n",
    "         houston: bool = False,\n",
    "         chunksize: int = 200000):\n",
    "    data1 = pd.read_csv(\"/project/lindner/air-pollution/current/2019/data-formatted/concat_aqs/Transformed_Data_48_201_0695.csv\")\n",
    "    data2 = pd.read_csv(\"/project/lindner/air-pollution/current/2019/data-formatted/concat_aqs/Transformed_Data_48_201_0416.csv\")\n",
    "    y = data2['o3']\n",
    "    data1 = data1.add_prefix('MoodyTowers_')\n",
    "    data2 = data2.add_prefix('ParkPlace_').drop(['o3', axis = 'columns'])\n",
    "    #Because of unneeded columns leftover from faulty script\n",
    "    data1X = data1.drop(['MoodyTowers_Unnamed: 0'], axis = 'columns').replace('48_201_0695', 0)\n",
    "    data2X = data2.drop(['ParkPlace_Unnamed: 0'], axis = 'columns').replace('48_201_0416', 1)\n",
    "    X = pd.concat([data1X,data2X], ignore_index=True).drop([\"MoodyTowers_Unnamed: 0.1\"], axis = 'columns')\n",
    "    #X, y = X.dropna(), y.dropna()\n",
    "    X, y = np.array([X,y])\n",
    "    scaler = MinMaxScaler()\n",
    "    X = BiScaler().fit_transform(X)\n",
    "    X = SoftImpute().fit_transform(X)\n",
    "    y = BiScaler().fit_transform(y)\n",
    "    y = SoftImpute().fit_transform(y)\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "    # Initialising the RNN\n",
    "    regressor = Sequential()\n",
    "    #Layers\n",
    "    regressor.add(Dense(25, input_dim=21, activation='relu', kernel_initializer='he_uniform'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(25, input_dim=21, activation='relu', kernel_initializer='he_uniform'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(25, input_dim=21, activation='relu', kernel_initializer='he_uniform'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    # Adding the output layer\n",
    "    regressor.add(Dense(units = 1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    # Fitting the RNN to the Training set\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=1)\n",
    "    # evaluate the model\n",
    "    train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "    # plot loss during training\n",
    "    pyplot.title('Loss / Mean Squared Error')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    pyplot.savefig(output_path+\"MSE_of_LSTM_model.png\")\n",
    "    regressor.save(output_path+\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
