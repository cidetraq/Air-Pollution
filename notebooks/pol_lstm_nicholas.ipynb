{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import pickle\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, LSTM, Input, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles={'cluster': {'in_path': '/project/lindner/moving/summer2018/2019/data-intermediate/', 'out_path': '/project/lindner/moving/summer2018/2019/models/'},\n",
    "         'nicholas': {'in_path': 'D:/programming-no-gdrive/air-pollution/data-intermediate/', 'out_path': 'D:/programming-no-gdrive/DASH/Air Pollution/models/'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user, window_stride, nd_window,load, epochs=10):\n",
    "    #Notebook\n",
    "    os.chdir('../python-scripts')\n",
    "    from format_data_labels import create_data_labels\n",
    "\n",
    "    # Average window_stride elements together to form a single row\n",
    "    sample_hours = window_stride / 12.0\n",
    "    print(\"Sample Hours: %f\" % sample_hours)\n",
    "    \n",
    "    # Number of future samples to mean for prediction\n",
    "    prediction_window = int(24 / sample_hours)\n",
    "    print(\"Prediction Window: %d\" % prediction_window)\n",
    "\n",
    "    # Length of the windowed sequence\n",
    "    sequence_length = int(7*24 / sample_hours)\n",
    "    print(\"Sequence Length: %d\" % sequence_length)\n",
    "\n",
    "    # Number of things we are doing regression to predict\n",
    "    num_outputs = 4\n",
    "    in_path=profiles[user]['in_path']\n",
    "    out_path=profiles[user]['out_path']\n",
    "    nd_window=pickle.load(open(in_path+nd_window, 'rb'))\n",
    "    data, labels=create_data_labels(window_stride, nd_window)\n",
    "    print('Created data and labels')\n",
    "    # Number of features we take from the data\n",
    "    print('Data shape: '+str(data.shape))\n",
    "    input_features = data.shape[2]\n",
    "    print('Sequence length: '+str(sequence_length))\n",
    "    print('Number of input features: '+str(input_features))\n",
    "    num_features = input_features\n",
    "    num_inputs = input_features\n",
    "    model_params=[sequence_length, input_features]\n",
    "    model=create_model(data, labels, out_path, model_params, num_outputs, load, epochs, window_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Theano\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_architecture(sequence_length, input_features, num_outputs, r2, window_stride):\n",
    "# For some reason putting some extra dimensions before an LSTM works wonders\n",
    "    layer_input = Input(shape=(sequence_length, input_features), name='inputs')\n",
    "    dense_1 = Dense(128, input_dim=(sequence_length, input_features))(layer_input)\n",
    "    layer_lstm = LSTM(64, return_sequences=True, dropout=0.5)(dense_1)\n",
    "    layer_flatten = Flatten()(layer_lstm)\n",
    "\n",
    "    layer_output = Dense(num_outputs, activation='linear', name='outputs')(layer_flatten)\n",
    "\n",
    "    model = Model(inputs=[layer_input], outputs=[layer_output])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[r2])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, labels, out_path, model_params,num_outputs,load,window_stride,epochs=10):\n",
    "    def r2(y_true, y_pred):\n",
    "        SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "        SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "        return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "    sequence_length=model_params[0]\n",
    "    input_features=model_params[1]\n",
    "\n",
    "    def sched(epoch, lr):\n",
    "        new_lr = 0.001 * (0.95 ** epoch)\n",
    "        print(\"Epoch(%d) LR: %f\" % (epoch+1, new_lr))\n",
    "        return new_lr\n",
    "\n",
    "    lr_decay = LearningRateScheduler(schedule=sched) \n",
    "\n",
    "    filepath='lstm_w'+str(window_stride)+'_f'+str(input_features)+'_o'+str(num_outputs)+'.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(out_path+filepath, monitor='val_r2', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir='./tb', histogram_freq=0, batch_size=128, write_graph=True, write_grads=False)\n",
    "    if load==None:\n",
    "        model=model_architecture(sequence_length, input_features, num_outputs, r2, window_stride)\n",
    "    else: \n",
    "        model=load_model(out_path+load, custom_objects={'r2': r2})\n",
    "    model.fit(x=data, y=labels, batch_size=128, epochs=epochs, validation_split=0.2, verbose=True, callbacks=[lr_decay, checkpoint, tensorboard])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-u\", '--user', type=str,\n",
    "                        help=\"cluster, nicholas, carroll\")\n",
    "    #parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2],\n",
    "    #                    help=\"increase output verbosity\")\n",
    "    parser.add_argument('-w', '--window_stride', type=int, help='window stride')\n",
    "    parser.add_argument('-f', '--filename', type=str, help='filename of nd_window')\n",
    "    parser.add_argument('-l', '--load_model', type=str, help='filename of model to load. if blank, will create new model.')\n",
    "    parser.add_argument('-e', '--epochs', type=int, help='number of epochs to train')\n",
    "    args = parser.parse_args()\n",
    "    main(args.user, args.window_stride, args.filename, args.load_model, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main('nicholas', 12, 'windowed_2000.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Plotting\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "for seq in range(0, data.shape[0] - sequence_length):\n",
    "\n",
    "    lookup = {'no': (0, 0), 'no2':(0, 1), 'nox':(1, 0), 'o3':(1, 1)}\n",
    "\n",
    "    pred = model.predict(data[seq].reshape(1, sequence_length, num_features))[0]\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "    for idx,f in enumerate([(4, 'no'), (5, 'no2'), (6, 'nox'), (7, 'o3')]):\n",
    "\n",
    "        feature_index, feature_name = f\n",
    "\n",
    "        X = []\n",
    "        Y_actual = []\n",
    "\n",
    "        for i in range(0, sequence_length + int(24*(1/sample_hours))):\n",
    "            X.append(seq+i)\n",
    "            Y_actual.append(data[seq+i][-1][feature_index])\n",
    "\n",
    "        Y_actual = np.array(Y_actual)\n",
    "\n",
    "        predicted_mean = pred[feature_index - 4]\n",
    "        actual_mean = np.mean(Y_actual[sequence_length:])\n",
    "        rolling_mean = np.mean(Y_actual[:sequence_length])\n",
    "        rolling_std = np.std(Y_actual[:sequence_length])\n",
    "\n",
    "        Y_pred = Y_actual.copy()\n",
    "        Y_pred[sequence_length:] = predicted_mean\n",
    "        Y_pred[:sequence_length] = np.nan\n",
    "\n",
    "        Y_actual_mean = Y_actual.copy()\n",
    "        Y_actual_mean[sequence_length:] = actual_mean\n",
    "        Y_actual_mean[:sequence_length] = np.nan\n",
    "\n",
    "        Y_rolling_mean = Y_actual.copy()\n",
    "        Y_rolling_mean[:sequence_length] = rolling_mean\n",
    "        Y_rolling_mean[sequence_length:] = np.nan\n",
    "\n",
    "        Y_rolling_std_upper = Y_actual.copy()\n",
    "        Y_rolling_std_upper[:sequence_length] = rolling_mean + rolling_std\n",
    "        Y_rolling_std_upper[sequence_length:] = np.nan\n",
    "\n",
    "        Y_rolling_std_lower = Y_actual.copy()\n",
    "        Y_rolling_std_lower[:sequence_length] = rolling_mean - rolling_std\n",
    "        Y_rolling_std_lower[sequence_length:] = np.nan   \n",
    "\n",
    "        subplot = ax[lookup[feature_name][0]][lookup[feature_name][1]]\n",
    "\n",
    "        subplot.plot(X, Y_actual, color='black', linewidth=4.0)\n",
    "        subplot.plot(X, Y_actual_mean, color='green', linewidth=4.0)\n",
    "        subplot.plot(X, Y_pred, color='purple', linewidth=4.0)\n",
    "        subplot.plot(X, Y_rolling_mean, color='green', linewidth=4.0)\n",
    "        subplot.plot(X, Y_rolling_std_upper, color='orange', linewidth=4.0)\n",
    "        subplot.plot(X, Y_rolling_std_lower, color='orange', linewidth=4.0)\n",
    "\n",
    "        subplot.grid()\n",
    "\n",
    "        subplot.set_title(\"%s 24 hour mean prediction\" % (feature_name,))\n",
    "\n",
    "        subplot.set_xlabel(\"Hours\")\n",
    "        subplot.set_ylabel(\"Scaled Concentration\")\n",
    "\n",
    "    fig.legend(['Actual Continuous', 'Actual Mean', 'Predicted Mean', 'Rolling Mean', 'Standard Deviation'])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig('charts/%.05d.png' % seq)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Rendered %d\" % seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
