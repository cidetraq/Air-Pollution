{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:/programming-no-gdrive/air-pollution/data-formatted/mpi-houston/Transformed_Data_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=df['epoch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=pd.read_csv(\"D:/programming-no-gdrive/DASH/Air Pollution/data-sample/2000_ds2_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=pd.read_csv(\"D:/programming-no-gdrive/DASH/Air Pollution/data-sample/2017_ds2_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "houston=pd.read_csv(\"D:/programming-no-gdrive/air-pollution/data-formatted/mpi-houston/Transformed_Data_2000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5285"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_procs=20\n",
    "epochs=np.array(houston['epoch'].unique())\n",
    "split_length=len(epochs)//(n_procs-1)\n",
    "epoch_splits=np.array_split(epochs, 19)\n",
    "len(epoch_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_series=houston['epoch'].unique()\n",
    "epoch_splits_series=np.array_split(epochs_series, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>48_201_1034_Latitude</th>\n",
       "      <th>48_201_1034_Longitude</th>\n",
       "      <th>48_201_1034_epoch</th>\n",
       "      <th>48_201_1034_no</th>\n",
       "      <th>48_201_1034_no2</th>\n",
       "      <th>48_201_1034_nox</th>\n",
       "      <th>48_201_1034_o3</th>\n",
       "      <th>...</th>\n",
       "      <th>48_201_1049_epoch</th>\n",
       "      <th>48_201_1049_no</th>\n",
       "      <th>48_201_1049_no2</th>\n",
       "      <th>48_201_1049_nox</th>\n",
       "      <th>48_201_1049_o3</th>\n",
       "      <th>48_201_1049_temp</th>\n",
       "      <th>48_201_1049_winddir</th>\n",
       "      <th>48_201_1049_windspd</th>\n",
       "      <th>48_201_1049_wind_x_dir</th>\n",
       "      <th>48_201_1049_wind_y_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946684800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.767997</td>\n",
       "      <td>-95.220582</td>\n",
       "      <td>946684800</td>\n",
       "      <td>75.99174</td>\n",
       "      <td>58.94147</td>\n",
       "      <td>134.50891</td>\n",
       "      <td>0.96883</td>\n",
       "      <td>...</td>\n",
       "      <td>946684800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946685100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.767997</td>\n",
       "      <td>-95.220582</td>\n",
       "      <td>946685100</td>\n",
       "      <td>73.94895</td>\n",
       "      <td>58.63369</td>\n",
       "      <td>132.15669</td>\n",
       "      <td>0.66288</td>\n",
       "      <td>...</td>\n",
       "      <td>946685100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946685400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.767997</td>\n",
       "      <td>-95.220582</td>\n",
       "      <td>946685400</td>\n",
       "      <td>76.09387</td>\n",
       "      <td>58.06941</td>\n",
       "      <td>133.99754</td>\n",
       "      <td>0.40793</td>\n",
       "      <td>...</td>\n",
       "      <td>946685400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       epoch  hour  day_of_year  48_201_1034_Latitude  48_201_1034_Longitude  \\\n",
       "0  946684800     0            1             29.767997             -95.220582   \n",
       "0  946685100     0            1             29.767997             -95.220582   \n",
       "0  946685400     0            1             29.767997             -95.220582   \n",
       "\n",
       "   48_201_1034_epoch  48_201_1034_no  48_201_1034_no2  48_201_1034_nox  \\\n",
       "0          946684800        75.99174         58.94147        134.50891   \n",
       "0          946685100        73.94895         58.63369        132.15669   \n",
       "0          946685400        76.09387         58.06941        133.99754   \n",
       "\n",
       "   48_201_1034_o3           ...            48_201_1049_epoch  48_201_1049_no  \\\n",
       "0         0.96883           ...                    946684800             NaN   \n",
       "0         0.66288           ...                    946685100             NaN   \n",
       "0         0.40793           ...                    946685400             NaN   \n",
       "\n",
       "   48_201_1049_no2  48_201_1049_nox  48_201_1049_o3  48_201_1049_temp  \\\n",
       "0              NaN              NaN             NaN               NaN   \n",
       "0              NaN              NaN             NaN               NaN   \n",
       "0              NaN              NaN             NaN               NaN   \n",
       "\n",
       "   48_201_1049_winddir  48_201_1049_windspd  48_201_1049_wind_x_dir  \\\n",
       "0                  NaN                  NaN                     NaN   \n",
       "0                  NaN                  NaN                     NaN   \n",
       "0                  NaN                  NaN                     NaN   \n",
       "\n",
       "   48_201_1049_wind_y_dir  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "\n",
       "[3 rows x 219 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=list(houston['epoch'].unique())\n",
    "columns=houston.columns\n",
    "reshaped_data=pd.DataFrame()\n",
    "for epoch in epochs[:3]:\n",
    "    subset=houston[houston['epoch']==epoch]\n",
    "    newrow=pd.DataFrame()\n",
    "    newrow['epoch']=pd.Series(epoch, index=[0])\n",
    "    newrow['hour']=pd.Series(subset.iloc[0]['hour'], index=[0])\n",
    "    newrow['day_of_year']=pd.Series(subset.iloc[0]['day_of_year'], index=[0])\n",
    "    for index, row in subset.iterrows():\n",
    "        aqs=row['AQS_Code']\n",
    "        for i, col in enumerate(row):\n",
    "            if i>0 and i<13:\n",
    "                newrow[aqs+\"_\"+columns[i]]=pd.Series(col, index=[0])     \n",
    "    reshaped_data=pd.concat([reshaped_data,newrow])\n",
    "reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008224"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(houston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2008224//100411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105696"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_length=epochs//(n_procs-1)\n",
    "split_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4016.448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2008.224*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.9408"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4016.448/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AQS_Code',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'epoch',\n",
       " 'no',\n",
       " 'no2',\n",
       " 'nox',\n",
       " 'o3',\n",
       " 'temp',\n",
       " 'winddir',\n",
       " 'windspd',\n",
       " 'wind_x_dir',\n",
       " 'wind_y_dir',\n",
       " 'hour',\n",
       " 'day_of_year']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=np.array(epochs)\n",
    "epoch_splits=np.split(epochs,144) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import plac\n",
    "import os\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds2_scheduler():\n",
    "    def __init__(self, n_procs, input_path: str,\n",
    "         input_prefix: str,\n",
    "         output_path: str,\n",
    "         year_begin: int,\n",
    "         year_end: int,\n",
    "         region: str):\n",
    "        self.n_procs=n_procs\n",
    "        self.input_path=input_path\n",
    "        self.input_prefix=input_prefix\n",
    "        self.years=range(year_begin,year_end)\n",
    "        self.region=region\n",
    "        self.output_path=output_path\n",
    "        self.jobs_queued=0\n",
    "        self.jobs_done=0\n",
    "        \n",
    "    def _schedule_container(self):\n",
    "        for year in self.years:\n",
    "            input_name = \"%s%d.csv\" % (self.input_prefix, year)\n",
    "            #output_name = 'ds2_'+region+'_'+str(year)\n",
    "            self.transform_year(year, input_name, self.input_path,self.output_path, self.region)\n",
    "            \n",
    "    def _transform_year(self, year, input_name, input_path, output_path, region):\n",
    "        df=pd.read_csv(input_path+input_name)\n",
    "        epochs=np.array(df['epoch'].unique())\n",
    "        #split_length=len(epochs)//(n_procs-1)\n",
    "        split_divisions=self.n_procs-1\n",
    "        epoch_splits=np.array_split(epochs, split_divisions)\n",
    "        for index,split in enumerate(epoch_splits):\n",
    "            job={'year': year, 'input_name': input_name, 'split_index': index, 'split_divisions': split_divisions}\n",
    "            comm.isend(job, dest=index, tag=1)\n",
    "            self.jobs_queued+=1\n",
    "            \n",
    "    def make_output_container(self):\n",
    "        dir_path=self.output_path+'partial/'\n",
    "        filenames=os.listdir(dir_path)\n",
    "        years=list(np.arange(year_begin,year_end))\n",
    "        for index,year in enumerate(years):\n",
    "            year_filenames=[filename for filename in filenames if str(year) in filename]\n",
    "            job={'year': year, 'filenames': year_filenames}\n",
    "            comm.isend(job, dest=index+1)\n",
    "            self.jobs_queued+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler=ds2_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ds2_worker():\n",
    "    def __init__(self, split_divisions: int, split_index: int, input_name: str, input_path: str,\n",
    "         output_path: str,\n",
    "         year: int,\n",
    "         region: str):\n",
    "        self.split_divisions=split_divisions\n",
    "        self.split_index=split_index\n",
    "        self.input_path=input_path\n",
    "        self.input_name=input_name\n",
    "        self.year=year\n",
    "        self.region=region\n",
    "        self.output_path=output_path\n",
    "        self.split, self.df=_load_df()\n",
    "        self.columns=df.columns\n",
    "        \n",
    "    def _load_df(self):\n",
    "        df=pd.read_csv(self.input_path+self.input_name)\n",
    "        epochs=np.array(df['epoch'].unique())\n",
    "        epoch_splits=np.split(epochs, self.split_divisions)\n",
    "        split=epoch_splits[self.split_index]\n",
    "        combined=pd.DataFrame()\n",
    "        for epoch in split:\n",
    "            sub=df[df['epoch']==epoch]\n",
    "            combined=pd.concat([combined, sub])\n",
    "        return split, combined\n",
    "    \n",
    "    def _transform(self):\n",
    "        newrows=pd.DataFrame()\n",
    "        for epoch in self.split:\n",
    "            subset=self.df[self.df['epoch']==epoch]\n",
    "            newrow=pd.DataFrame()\n",
    "            newrow['epoch']=pd.Series(epoch, index=[0])\n",
    "            newrow['hour']=pd.Series(subset.iloc[0]['hour'], index=[0])\n",
    "            newrow['day_of_year']=pd.Series(subset.iloc[0]['day_of_year'], index=[0])\n",
    "            for index, row in subset.iterrows():\n",
    "                aqs=row['AQS_Code']\n",
    "                for i, col in enumerate(row):\n",
    "                    if i>0 and i<13:\n",
    "                        newrow[aqs+\"_\"+self.columns[i]]=pd.Series(col, index=[0])     \n",
    "            #print(newrow)\n",
    "            newrows=pd.concat([newrows, newrow])\n",
    "        return newrows   \n",
    "    \n",
    "    def _save(self, newrows):\n",
    "        newrows.to_csv(self.output_path+'partial/'+'ds2_split_'+str(self.split_index)+'_'+self.region+'_'+str(self.year)+'.csv', index=False)\n",
    "        \n",
    "    def _run(self):\n",
    "        print(\"Got job: %s_%s\" % (self.split_index, self.year))\n",
    "        newrows=ds2_worker._transform()\n",
    "        ds2_worker._save(newrows)\n",
    "        print(\"Finished job: %s_%s\" % (self.split_index, self.year))\n",
    "        \n",
    "    def make_output(self, job):\n",
    "        reshaped_data=pd.DataFrame()\n",
    "        year=job['year']\n",
    "        for year_file in job['filenames']:\n",
    "            df=pd.read_csv(output_path+'/partial/'+year_file)\n",
    "            reshaped_data=pd.concat([reshaped_data, df])\n",
    "        reshaped_data.to_csv(output_path+'ds2_'+region+'_'+str(year)+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker=ds2_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "plac.annotations(\n",
    "        input_path=(\"Path containing the data files to ingest\", \"option\", \"P\", str),\n",
    "        input_prefix=(\"{$prefix}year.csv\", \"option\", \"p\", str),\n",
    "        input_suffix=(\"year{$suffix}.csv\", \"option\", \"s\", str),\n",
    "        output_path=(\"Path to write the resulting numpy sequences / transform cache\", \"option\", \"o\", str),\n",
    "        year_begin=(\"First year to process\", \"option\", \"b\", int),\n",
    "        year_end=(\"Year to stop with\", \"option\", \"e\", int),\n",
    "        masknan=(\"Mask nan rows instead of dropping them\", \"option\", \"M\", float),\n",
    "        region=('Region of Texas. Default: houston', 'option', \"r\", str),\n",
    "        skip_to_output=('Skip to the concatenation of long rows and output step. Default: False', 'option', 'ff', bool)\n",
    ")\n",
    "def main(input_path: str = '/project/lindner/moving/summer2018/2019/data-formatted/mpi-houston/',\n",
    "         input_prefix: str = \"Transformed_Data_\",\n",
    "         input_suffix: str = \"\",\n",
    "         output_path: str = '/project/lindner/moving/summer2018/2019/data-formatted/mpi-houston/ds2/',\n",
    "         year_begin: int = 2000,\n",
    "         year_end: int = 2001,\n",
    "         masknan: float = None,\n",
    "         region: str= \"houston\",\n",
    "         skip_to_output: bool=False):\n",
    "\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    n_procs = comm.Get_size()\n",
    "    \n",
    "    if rank == 0 and skip_to_output==False:\n",
    "        scheduler=ds2_scheduler(nprocs, input_path=input_path,\n",
    "         input_prefix=input_prefix,\n",
    "         output_path=output_path,\n",
    "         year_begin=year_begin,\n",
    "         year_end=year_end,\n",
    "         region=region)\n",
    "        scheduler.schedule_container() \n",
    "    while scheduler.jobs_done<(n_procs-1):\n",
    "        #Update jobs_done counter, no other functionality\n",
    "        req = comm.irecv()\n",
    "        data = req.wait()\n",
    "        scheduler.jobs_done+=1\n",
    "        \n",
    "    if rank > 0 and skip_to_output==False:\n",
    "        req=comm.irecv()\n",
    "        job=req.wait()\n",
    "        if job['split_divisions']:\n",
    "            worker=ds2_worker(split_divisions=job['split_divisions'], split_index=job['split_index'], input_name=job['split_index'], input_path=input_path,\n",
    "             output_path=output_path,\n",
    "             year=job['year'],\n",
    "             region=region)\n",
    "            worker._run()\n",
    "            #Send information to scheduler to update jobs_done\n",
    "            req = comm.isend({'job': True, 'rank': rank}, dest=0)\n",
    "            req.wait()\n",
    "        else:\n",
    "            output_step=True\n",
    "    \n",
    "    if rank == 0:\n",
    "        scheduler.make_output_container()\n",
    "        \n",
    "    if rank > 0 or output_step==True:\n",
    "        req=comm.irecv()\n",
    "        job=req.wait()\n",
    "        print(\"Got job: output_%s\" % (job['year']))\n",
    "        worker.make_output(job)\n",
    "        print(\"Finished job: output_%s, shutting down...\" % (job['year']))\n",
    "        result={'job': True}\n",
    "        req = comm.isend(result, dest=0)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nprocs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-b8415382d7b1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(input_path, input_prefix, input_suffix, output_path, year_begin, year_end, masknan, region, skip_to_output)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mskip_to_output\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         scheduler=ds2_scheduler(nprocs, input_path=input_path,\n\u001b[0m\u001b[0;32m     28\u001b[0m          \u001b[0minput_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m          \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nprocs' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(2000,2001))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
